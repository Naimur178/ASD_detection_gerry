{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1. Import Needed Modules](#import) ##\n",
    "## [2. Read in images and create a dataframe of image paths and class labels](#makedf) ## \n",
    "## [3. Balance the trainning set](#Balance) ##\n",
    "## [4. Create train, test and validation generators](#generators) ## \n",
    "## [5. Create a function to show Training Image Samples](#show) ## \n",
    "## [6. Create the Model](#model) ## \n",
    "## [7. Create a custom Keras callback to continue or halt training](#callback) ## \n",
    "## [8. Instantiate custom callback ](#callbacks) ##\n",
    "## [9. Train the model](#train) ##\n",
    "## [10. Define a function to plot the training data](#plot) ##\n",
    "## [11. Make predictions on test set, create Confusion Matrix and Classification Report](#result) ##\n",
    "## [12 Save the model](#save) ##\n",
    "## [13 Plot Validation Loss % change between epochs](#implot) ##\n",
    "## [14 Analysis of the Validation Loss % change plot](#analysis) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "# <center>Import Need Modules</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:02.151185Z",
     "iopub.status.busy": "2022-07-31T23:58:02.149962Z",
     "iopub.status.idle": "2022-07-31T23:58:09.018104Z",
     "shell.execute_reply": "2022-07-31T23:58:09.017118Z",
     "shell.execute_reply.started": "2022-07-31T23:58:02.150375Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"makedf\"></a>\n",
    "# <center>Read in train, test and valid images and create train, test and validation data frames</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes():\n",
    "    print('Enter the full path to the directory containing the train, test and valid sub directories')\n",
    "    sdir=input(' ')\n",
    "    dataset_names= ['train', 'test' ,'valid']\n",
    "    train_path=os.path.join(sdir, 'train')\n",
    "    test_path=os.path.join(sdir, 'test')\n",
    "    valid_path=os.path.join(sdir, 'valid')\n",
    "    path_list=[train_path, test_path, valid_path]   \n",
    "    zip_list=zip(dataset_names, path_list)\n",
    "    \n",
    "    # generate train_df, test_df, valid_df\n",
    "    for dataset, setpath in zip_list:\n",
    "        filepaths=[]\n",
    "        labels=[]\n",
    "        classes=sorted(os.listdir(setpath))\n",
    "        for klass in classes:\n",
    "            classpath=os.path.join(setpath, klass)\n",
    "            flist=sorted(os.listdir(classpath))           \n",
    "            desc=f'{dataset:6s}-{klass:13s}'            \n",
    "            for f in tqdm(flist, ncols=130, desc=desc):\n",
    "                fpath=os.path.join(classpath,f)\n",
    "                filepaths.append(fpath)\n",
    "                labels.append(klass)\n",
    "        Fseries=pd.Series(filepaths, name='filepaths')\n",
    "        Lseries=pd.Series(labels, name='labels')\n",
    "        df=pd.concat([Fseries, Lseries], axis=1)\n",
    "        if dataset =='train':\n",
    "            train_df=df\n",
    "        elif dataset == 'test':\n",
    "            test_df=df\n",
    "        else:\n",
    "            valid_df=df\n",
    "    classes=sorted(train_df['labels'].unique())\n",
    "    class_count=len(classes)\n",
    "    sample_df=train_df.sample(n=50, replace=False)\n",
    "    # calculate the average image height and with\n",
    "    ht=0\n",
    "    wt=0\n",
    "    count=0\n",
    "    for i in range(len(sample_df)):\n",
    "        fpath=sample_df['filepaths'].iloc[i]\n",
    "        try:\n",
    "            img=cv2.imread(fpath)\n",
    "            h=img.shape[0]\n",
    "            w=img.shape[1]\n",
    "            wt +=w\n",
    "            ht +=h\n",
    "            count +=1\n",
    "        except:\n",
    "            pass\n",
    "    have=int(ht/count)\n",
    "    wave=int(wt/count)\n",
    "    aspect_ratio=have/wave\n",
    "    print('number of classes in processed dataset= ', class_count)    \n",
    "    counts=list(train_df['labels'].value_counts())    \n",
    "    print('the maximum files in any class in train_df is ', max(counts), '  the minimum files in any class in train_df is ', min(counts))\n",
    "    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \n",
    "    print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n",
    "    return train_df, test_df, valid_df, classes, class_count, sdir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:09.042837Z",
     "iopub.status.busy": "2022-07-31T23:58:09.042029Z",
     "iopub.status.idle": "2022-07-31T23:58:45.158642Z",
     "shell.execute_reply": "2022-07-31T23:58:45.157634Z",
     "shell.execute_reply.started": "2022-07-31T23:58:09.042801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the full path to the directory containing the train, test and valid sub directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train -autistic     : 100%|███████████████████████████████████████████████████████████████| 1263/1263 [00:00<00:00, 252750.89it/s]\n",
      "train -non_autistic : 100%|███████████████████████████████████████████████████████████████| 1263/1263 [00:00<00:00, 250551.29it/s]\n",
      "test  -autistic     : 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50189.11it/s]\n",
      "test  -non_autistic : 100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "valid -autistic     : 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 99461.80it/s]\n",
      "valid -non_autistic : 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 49896.55it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\naimu\\Documents\\Thesis\\Autism by image\\autism\\autism.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_df, test_df, valid_df, classes, class_count, sdir\u001b[39m=\u001b[39mmake_dataframes()\n",
      "\u001b[1;32mc:\\Users\\naimu\\Documents\\Thesis\\Autism by image\\autism\\autism.ipynb Cell 6\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m have\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(ht\u001b[39m/\u001b[39;49mcount)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m wave\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(wt\u001b[39m/\u001b[39mcount)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naimu/Documents/Thesis/Autism%20by%20image/autism/autism.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m aspect_ratio\u001b[39m=\u001b[39mhave\u001b[39m/\u001b[39mwave\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train_df, test_df, valid_df, classes, class_count, sdir=make_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generators\"></a>\n",
    "# <center>Create the train_gen, test_gen final_test_gen and valid_gen</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:04:36.886240Z",
     "iopub.status.busy": "2022-08-01T00:04:36.885533Z",
     "iopub.status.idle": "2022-08-01T00:05:00.064109Z",
     "shell.execute_reply": "2022-08-01T00:05:00.062813Z",
     "shell.execute_reply.started": "2022-08-01T00:04:36.886201Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m batch_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m\n\u001b[0;32m     32\u001b[0m img_size\u001b[39m=\u001b[39m(\u001b[39m200\u001b[39m,\u001b[39m200\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes\u001b[39m=\u001b[39mmake_gens(batch_size, train_df, test_df, valid_df, img_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def make_gens(batch_size, train_df, test_df, valid_df, img_size):\n",
    "    trgen=ImageDataGenerator()\n",
    "    #trgen=ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20 )\n",
    "    t_and_v_gen=ImageDataGenerator()\n",
    "    msg='{0:70s} for train generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "    msg='{0:70s} for valid generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "    # for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n",
    "    # this insures that we go through all the sample in the test set exactly once.\n",
    "    length=len(test_df)\n",
    "    test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "    test_steps=int(length/test_batch_size)\n",
    "    msg='{0:70s} for test generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "    # from the generator we can get information we will need later\n",
    "    classes=list(train_gen.class_indices.keys())\n",
    "    class_indices=list(train_gen.class_indices.values())\n",
    "    class_count=len(classes)\n",
    "    labels=test_gen.labels\n",
    "    print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n",
    "    return train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes\n",
    "\n",
    "\n",
    "batch_size=20\n",
    "img_size=(200,200)\n",
    "train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes=make_gens(batch_size, train_df, test_df, valid_df, img_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a text file to the sdir that contains the classes information. This file named classes.txt. It can be used in association with  \n",
    "the averaging_predictor.ipynb notebook. That notebook needs to know the classes information to make predictions on image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m content\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(classes)\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     content\u001b[39m=\u001b[39mcontent\u001b[39m.\u001b[39mreplace(char, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "content=str(classes)\n",
    "for char in ['\\'', '[', ']', ' ']:\n",
    "    content=content.replace(char, '')\n",
    "wpath=os.path.join(sdir,'classes.txt')\n",
    "with open(wpath , 'w') as f:\n",
    "    f.write(content)\n",
    "# the file is stored in the sdir as classes.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"show\"></a>\n",
    "# <center>Create a function to show example training images</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:05:00.067667Z",
     "iopub.status.busy": "2022-08-01T00:05:00.065344Z",
     "iopub.status.idle": "2022-08-01T00:05:02.905615Z",
     "shell.execute_reply": "2022-08-01T00:05:02.904541Z",
     "shell.execute_reply.started": "2022-08-01T00:05:00.067628Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m         plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> 21\u001b[0m show_image_samples(train_gen )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_gen' is not defined"
     ]
    }
   ],
   "source": [
    "def show_image_samples(gen ):\n",
    "    t_dict=gen.class_indices\n",
    "    classes=list(t_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(25, 25))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):        \n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i] /255       \n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=18)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_image_samples(train_gen )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "# <center>Create a model using transfer learning with EfficientNetB5</center>\n",
    "### NOTE experts advise you make the base model initially not trainable when you do transfer learning.   \n",
    "### Then train for some number of epochs then fine tune model by making base model trainable and run more epochs\n",
    "### I have found this to be WRONG!!!!\n",
    "### Making the base model trainable from the outset leads to faster convegence and a lower validation loss\n",
    "### for the same number of total epochs! Insure  you initialize the transfer model with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:05:02.907293Z",
     "iopub.status.busy": "2022-08-01T00:05:02.906872Z",
     "iopub.status.idle": "2022-08-01T00:05:08.963048Z",
     "shell.execute_reply": "2022-08-01T00:05:08.962043Z",
     "shell.execute_reply.started": "2022-08-01T00:05:02.907249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the initial model learning rate. I recommend .002\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941136/43941136 [==============================] - 6s 0us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     model\u001b[39m.\u001b[39mcompile(Adamax(learning_rate\u001b[39m=\u001b[39mlr), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m---> 18\u001b[0m model\u001b[39m=\u001b[39mmake_model(img_size)\n",
      "Cell \u001b[1;32mIn [14], line 13\u001b[0m, in \u001b[0;36mmake_model\u001b[1;34m(img_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[39m=\u001b[39m Dense(\u001b[39m256\u001b[39m, kernel_regularizer \u001b[39m=\u001b[39m regularizers\u001b[39m.\u001b[39ml2(l \u001b[39m=\u001b[39m \u001b[39m0.016\u001b[39m),activity_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39ml1(\u001b[39m0.006\u001b[39m),\n\u001b[0;32m     11\u001b[0m                 bias_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39ml1(\u001b[39m0.006\u001b[39m) ,activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[0;32m     12\u001b[0m x\u001b[39m=\u001b[39mDropout(rate\u001b[39m=\u001b[39m\u001b[39m.4\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m123\u001b[39m)(x)       \n\u001b[1;32m---> 13\u001b[0m output\u001b[39m=\u001b[39mDense(class_count, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[0;32m     14\u001b[0m model\u001b[39m=\u001b[39mModel(inputs\u001b[39m=\u001b[39mbase_model\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39moutput)\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39mcompile(Adamax(learning_rate\u001b[39m=\u001b[39mlr), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_count' is not defined"
     ]
    }
   ],
   "source": [
    "def make_model(img_size):  \n",
    "    print('Enter the initial model learning rate. I recommend .002', flush=True)\n",
    "    lr=float(input(' '))\n",
    "    img_shape=(img_size[0], img_size[1], 3)\n",
    "    base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
    "    # Note you are always told NOT to make the base model trainable initially- that is WRONG you get better results leaving it trainable\n",
    "    base_model.trainable=True\n",
    "    x=base_model.output\n",
    "    x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "    x=Dropout(rate=.4, seed=123)(x)       \n",
    "    output=Dense(class_count, activation='softmax')(x)\n",
    "    model=Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "model=make_model(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"callback\"></a>\n",
    "# <center>Create a custom Keras callback to continue and optionally set LR or halt training</center>\n",
    "The LR_ASK callback is a convenient callback that allows you to continue training for ask_epoch more epochs or to halt training.  \n",
    "If you elect to continue training for more epochs you are given the option to retain the current learning rate (LR) or to  \n",
    "enter a new value for the learning rate. The form of use is:  \n",
    "ask=LR_ASK(model,epochs, ask_epoch) where:  \n",
    "* model is a string which is the name of your compiled model\n",
    "* epochs is an integer which is the number of epochs to run specified in model.fit\n",
    "* ask_epoch is an integer. If ask_epoch is set to a value say 5 then the model will train for 5 epochs.  \n",
    "  then the user is ask to enter H to halt training, or enter an inter value. For example if you enter 4  \n",
    "  training will continue for 4 more epochs to epoch 9 then you will be queried again. Once you enter an  \n",
    "  integer value you are prompted to press ENTER to continue training using the current learning rate  \n",
    "  or to enter a new value for the learning rate.\n",
    " * dwell is a boolean. If set to true the function compares the validation loss for the current tp the lowest   \n",
    "   validation loss thus far achieved. If the validation loss for the current epoch is larger then learning rate  \n",
    "   is automatically adjust by the formulanew_lr=lr * factor where factor is a float between 0 and 1. The motivation  \n",
    "   here is that if the validatio loss increased we have moved to a point in Nspace on the cost functiob surface that  \n",
    "   if less favorable(higher cost) than for the epoch with the lowest cost. So the model is loaded with the weights\n",
    "   from the epoch with the lowest loss and the learning rate is reduced\n",
    "  \n",
    " At the end of training the model weights are set to the weights for the epoch that achieved the lowest validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:05:08.964770Z",
     "iopub.status.busy": "2022-08-01T00:05:08.964407Z",
     "iopub.status.idle": "2022-08-01T00:05:08.991685Z",
     "shell.execute_reply": "2022-08-01T00:05:08.989705Z",
     "shell.execute_reply.started": "2022-08-01T00:05:08.964735Z"
    }
   },
   "outputs": [],
   "source": [
    "class LR_ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch, dwell=True, factor=.4): # initialization of the callback\n",
    "        super(LR_ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        self.lowest_vloss=np.inf\n",
    "        self.lowest_aloss=np.inf\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.best_epoch=1\n",
    "        self.plist=[]\n",
    "        self.alist=[]\n",
    "        self.dwell= dwell\n",
    "        self.factor=factor\n",
    "        \n",
    "    def get_list(self): # define a function to return the list of % validation change\n",
    "        return self.plist, self.alist\n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n",
    "            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "        \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training  \n",
    "        print('loading model with weights from epoch ', self.best_epoch)\n",
    "        model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print (msg, flush=True) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        vloss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        aloss=logs.get('loss')\n",
    "        if epoch >0:\n",
    "            deltav = self.lowest_vloss- vloss \n",
    "            pimprov=(deltav/self.lowest_vloss) * 100 \n",
    "            self.plist.append(pimprov)\n",
    "            deltaa=self.lowest_aloss-aloss\n",
    "            aimprov=(deltaa/self.lowest_aloss) * 100\n",
    "            self.alist.append(aimprov)\n",
    "        else:\n",
    "            pimprov=0.0 \n",
    "            aimprov=0.0\n",
    "        if vloss< self.lowest_vloss:\n",
    "            self.lowest_vloss=vloss\n",
    "            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "            self.best_epoch=epoch + 1\n",
    "            \n",
    "            print (f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights')\n",
    "        else: # validation loss increased\n",
    "            pimprov=abs(pimprov)\n",
    "            print (f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights')\n",
    "            if self.dwell: # if dwell is True when the validation loss increases the learning rate is automatically reduced and model weights are set to best weights\n",
    "                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                new_lr=lr * self.factor\n",
    "                print(f'learning rate was automatically adjusted from {lr:8.6f} to {new_lr:8.6f}')\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                model.set_weights(self.best_weights) # set the weights of the model to the best weights      \n",
    "                \n",
    "        if aloss< self.lowest_aloss:\n",
    "            self.lowest_aloss=aloss        \n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)\n",
    "                        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                        print(f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR')\n",
    "                        ans=input(' ')\n",
    "                        if ans =='':\n",
    "                            print (f'keeping current LR of {lr:7.5f}')\n",
    "                        else:\n",
    "                            new_lr=float(ans)\n",
    "                            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                            print(' changing LR to ', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"callbacks\"></a>\n",
    "# <center>Instantiate custom callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:05:08.993391Z",
     "iopub.status.busy": "2022-08-01T00:05:08.992959Z",
     "iopub.status.idle": "2022-08-01T00:05:09.227435Z",
     "shell.execute_reply": "2022-08-01T00:05:09.226459Z",
     "shell.execute_reply.started": "2022-08-01T00:05:08.993353Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m\n\u001b[0;32m      2\u001b[0m ask_epoch\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m ask\u001b[39m=\u001b[39mLR_ASK(model, epochs,  ask_epoch)\n\u001b[0;32m      4\u001b[0m \u001b[39m#rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,verbose=1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#callbacks=[rlronp, ask]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m callbacks\u001b[39m=\u001b[39m[ask]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "epochs=40\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch)\n",
    "#rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,verbose=1)\n",
    "#callbacks=[rlronp, ask]\n",
    "callbacks=[ask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# <center>Train the model\n",
    "### Note unlike how you are told it is BETTER to make the base model trainable from the outset if you are doing transfer learning\n",
    "### The model will converge faster and have a lower validation losss. Ensure you initialize the transfer model with imagenet weights.  \n",
    "### I have done a lot of testing running both ways hand have alwats found this to be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:05:09.231675Z",
     "iopub.status.busy": "2022-08-01T00:05:09.230857Z",
     "iopub.status.idle": "2022-08-01T00:28:38.778770Z",
     "shell.execute_reply": "2022-08-01T00:28:38.777808Z",
     "shell.execute_reply.started": "2022-08-01T00:05:09.231644Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39mtrain_gen,  epochs\u001b[39m=\u001b[39mepochs, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, callbacks\u001b[39m=\u001b[39mcallbacks,  validation_data\u001b[39m=\u001b[39mvalid_gen,\n\u001b[0;32m      2\u001b[0m                validation_steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  initial_epoch\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot\"></a>\n",
    "# <center>Define a function to plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:28:38.780425Z",
     "iopub.status.busy": "2022-08-01T00:28:38.780023Z",
     "iopub.status.idle": "2022-08-01T00:28:39.361315Z",
     "shell.execute_reply": "2022-08-01T00:28:39.360363Z",
     "shell.execute_reply.started": "2022-08-01T00:28:38.780354Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m index_loss\n\u001b[1;32m---> 39\u001b[0m loss_index\u001b[39m=\u001b[39mtr_plot(history,\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(25,10))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].scatter(Epochs, tloss, s=100, c='red')    \n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[0].set_ylabel('Loss', fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].scatter(Epochs, tacc, s=100, c='red')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=18)\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    return index_loss\n",
    "    \n",
    "loss_index=tr_plot(history,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"result\"></a>\n",
    "# <center>Make Predictions on the test set</a>\n",
    "### Define a function which takes in a test generator and an integer test_steps\n",
    "### and generates predictions on the test set including a confusion matric\n",
    "### and a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:28:39.363735Z",
     "iopub.status.busy": "2022-08-01T00:28:39.363035Z",
     "iopub.status.idle": "2022-08-01T00:28:52.883345Z",
     "shell.execute_reply": "2022-08-01T00:28:52.881743Z",
     "shell.execute_reply.started": "2022-08-01T00:28:39.363696Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClassification Report:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----------------------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, clr)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m errors, tests\n\u001b[1;32m---> 35\u001b[0m errors, tests\u001b[39m=\u001b[39mpredictor(test_gen, test_steps)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_gen' is not defined"
     ]
    }
   ],
   "source": [
    "def predictor(test_gen, test_steps):\n",
    "    y_pred= []\n",
    "    y_true=test_gen.labels\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    errors=0\n",
    "    preds=model.predict(test_gen, verbose=1)\n",
    "    tests=len(preds)    \n",
    "    for i, p in enumerate(preds):        \n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "            file=test_gen.filenames[i]            \n",
    "        y_pred.append(pred_index)\n",
    "            \n",
    "    acc=( 1-errors/tests) * 100\n",
    "    print(f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}')\n",
    "    ypred=np.array(y_pred)\n",
    "    ytrue=np.array(y_true)\n",
    "    if class_count <=30:\n",
    "        cm = confusion_matrix(ytrue, ypred )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    return errors, tests\n",
    "errors, tests=predictor(test_gen, test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved an F1 score of 83,6% which is not to bad considering we limited the number of images in train_df to 150  \n",
    "images per class and reduced the image size to 200 X 282. This was done to reduce training time at the expense of the  \n",
    "F1 score. Did model did better than I expected given that the labels for the images were probably done by a human or  \n",
    "humans based on a visual rather than an analytic criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "# <center>Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:28:52.887125Z",
     "iopub.status.busy": "2022-08-01T00:28:52.886163Z",
     "iopub.status.idle": "2022-08-01T00:28:54.180872Z",
     "shell.execute_reply": "2022-08-01T00:28:54.179775Z",
     "shell.execute_reply.started": "2022-08-01T00:28:52.887072Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m working_dir\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTemp\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mAutism\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m subject\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mautism-B3\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m----> 3\u001b[0m acc\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(( \u001b[39m1\u001b[39m\u001b[39m-\u001b[39merrors\u001b[39m/\u001b[39mtests) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m index\u001b[39m=\u001b[39macc\u001b[39m.\u001b[39mrfind(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m acc\u001b[39m=\u001b[39macc[:index \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'errors' is not defined"
     ]
    }
   ],
   "source": [
    "working_dir=r'C:\\Temp\\Autism'\n",
    "subject='autism-B3' \n",
    "acc=str(( 1-errors/tests) * 100)\n",
    "index=acc.rfind('.')\n",
    "acc=acc[:index + 3]\n",
    "save_id= subject + '_' + str(acc) + '.h5' \n",
    "model_save_loc=os.path.join(working_dir, save_id)\n",
    "model.save(model_save_loc)\n",
    "print ('model was saved as ' , model_save_loc ) \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ccd81cb438304a1e6e09c8329ddcbddb6bab06b5bd0aab6fea11efabe6b1268"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
